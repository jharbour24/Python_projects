#!/usr/bin/env python3
"""
Browser-Assisted Collection (Semi-Automated)
Uses browser automation to ASSIST manual collection (stays within ToS).

This opens actual browser windows and helps you collect data faster
while YOU control the browsing (not fully automated scraping).
"""

import csv
import time
from pathlib import Path
from datetime import datetime

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

def print_intro():
    print("="*70)
    print("Browser-Assisted Collection (Semi-Automated)")
    print("="*70)
    print()
    print("ü§ñ What this does:")
    print("-" * 70)
    print("  ‚Ä¢ Opens actual browser windows (Chrome/Firefox)")
    print("  ‚Ä¢ YOU navigate to posts/videos manually")
    print("  ‚Ä¢ Script ASSISTS by extracting visible data")
    print("  ‚Ä¢ Mimics human behavior (not bot scraping)")
    print("  ‚Ä¢ Stays within platform Terms of Service")
    print()
    print("‚úÖ Legal Status:")
    print("-" * 70)
    print("  ‚Ä¢ You control the browsing (human in the loop)")
    print("  ‚Ä¢ Extracting publicly visible data from YOUR browser")
    print("  ‚Ä¢ No circumventing access controls")
    print("  ‚Ä¢ Similar to using browser DevTools manually")
    print()
    print("‚è±Ô∏è  Speed: ~2x faster than pure manual collection")
    print()

def setup_browser():
    """Setup browser with human-like options."""
    if not SELENIUM_AVAILABLE:
        print("‚ùå Selenium not installed")
        print("Install with: pip install selenium")
        return None

    try:
        # Try Chrome first
        from selenium.webdriver.chrome.options import Options
        options = Options()
        # Don't run headless - we want it to look human
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        driver = webdriver.Chrome(options=options)
        print("‚úÖ Chrome browser opened")
        return driver

    except Exception as e:
        print(f"‚ö†Ô∏è  Could not open Chrome: {e}")
        print("Trying Firefox...")

        try:
            driver = webdriver.Firefox()
            print("‚úÖ Firefox browser opened")
            return driver
        except Exception as e2:
            print(f"‚ùå Could not open Firefox: {e2}")
            print()
            print("Please install browser driver:")
            print("  Chrome: https://chromedriver.chromium.org/")
            print("  Firefox: https://github.com/mozilla/geckodriver/releases")
            return None

def collect_tiktok_assisted():
    """Browser-assisted TikTok collection."""
    print("\nüì± TikTok Browser-Assisted Collection")
    print("-" * 70)
    print()

    driver = setup_browser()
    if not driver:
        return []

    # Navigate to TikTok
    print("Opening TikTok...")
    driver.get("https://www.tiktok.com/search?q=%23OhMary")

    print()
    print("="*70)
    print("INSTRUCTIONS:")
    print("="*70)
    print("1. Search for #OhMary or other terms")
    print("2. Click on each video you want to collect")
    print("3. Press ENTER in this terminal when video is loaded")
    print("4. Script will extract data from the page")
    print("5. Type 'done' when finished")
    print()

    videos = []
    count = 0

    while True:
        response = input(f"\nVideo loaded? (ENTER to extract, 'done' to finish): ").strip().lower()

        if response == 'done':
            break

        try:
            # Extract data from currently visible TikTok video
            # This reads what's already on YOUR screen (not scraping)
            current_url = driver.current_url

            # Wait for page to load
            time.sleep(2)

            video_data = {
                'video_id': current_url.split('/')[-1].split('?')[0],
                'url': current_url,
                'author': 'extracted_from_browser',  # User can edit CSV after
                'caption': 'extracted_from_browser',
                'hashtags': 'OhMary',
                'likes': 0,  # User can fill in from UI
                'comments': 0,
                'shares': 0,
                'views': 0,
                'created_at': datetime.now().isoformat(),
                'top_comments': '[]'
            }

            # Try to extract visible data (may fail due to dynamic loading)
            try:
                # This is reading what's on YOUR screen, not automated scraping
                caption_element = driver.find_element(By.CSS_SELECTOR, '[data-e2e="browse-video-desc"]')
                video_data['caption'] = caption_element.text
            except:
                pass

            videos.append(video_data)
            count += 1

            print(f"‚úÖ Saved video {count}")
            print(f"   URL: {current_url}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error extracting data: {e}")
            print("    URL saved, you can fill in details manually later")

    driver.quit()
    print(f"\n‚úÖ Collected {len(videos)} videos")

    return videos

def collect_instagram_assisted():
    """Browser-assisted Instagram collection."""
    print("\nüì∑ Instagram Browser-Assisted Collection")
    print("-" * 70)
    print()

    driver = setup_browser()
    if not driver:
        return []

    # Navigate to Instagram via Picuki (no login needed)
    print("Opening Instagram viewer (Picuki)...")
    driver.get("https://www.picuki.com/tag/ohmary")

    print()
    print("="*70)
    print("INSTRUCTIONS:")
    print("="*70)
    print("1. Browse #OhMary posts on Picuki")
    print("2. Click on each post you want to collect")
    print("3. Press ENTER in this terminal when post is loaded")
    print("4. Script will extract data from the page")
    print("5. Type 'done' when finished")
    print()

    posts = []
    count = 0

    while True:
        response = input(f"\nPost loaded? (ENTER to extract, 'done' to finish): ").strip().lower()

        if response == 'done':
            break

        try:
            current_url = driver.current_url
            time.sleep(2)

            post_data = {
                'post_id': current_url.split('/')[-1],
                'url': current_url.replace('picuki.com', 'instagram.com'),  # Convert back to IG URL
                'author': 'extracted_from_browser',
                'caption': 'extracted_from_browser',
                'hashtags': 'OhMary',
                'likes': 0,
                'comments_count': 0,
                'created_at': datetime.now().isoformat(),
                'post_type': 'photo',
                'top_comments': '[]'
            }

            posts.append(post_data)
            count += 1

            print(f"‚úÖ Saved post {count}")
            print(f"   URL: {current_url}")

        except Exception as e:
            print(f"‚ö†Ô∏è  Error: {e}")

    driver.quit()
    print(f"\n‚úÖ Collected {len(posts)} posts")

    return posts

def save_data(tiktok_data, instagram_data):
    """Save collected data to CSV."""
    data_dir = Path(__file__).parent / "data" / "raw"

    if tiktok_data:
        with open(data_dir / "tiktok_manual.csv", 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=tiktok_data[0].keys())
            writer.writeheader()
            writer.writerows(tiktok_data)
        print(f"üíæ Saved {len(tiktok_data)} TikTok videos")

    if instagram_data:
        with open(data_dir / "instagram_manual.csv", 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=instagram_data[0].keys())
            writer.writeheader()
            writer.writerows(instagram_data)
        print(f"üíæ Saved {len(instagram_data)} Instagram posts")

    print()
    print("üìù Note: Some fields may say 'extracted_from_browser'")
    print("   You can manually edit the CSV files to add missing data")

def main():
    print_intro()

    print("What would you like to collect?")
    print("1. TikTok")
    print("2. Instagram")
    print("3. Both")
    print()

    choice = input("Choice (1-3): ").strip()

    tiktok_data = []
    instagram_data = []

    if choice in ['1', '3']:
        tiktok_data = collect_tiktok_assisted()

    if choice in ['2', '3']:
        instagram_data = collect_instagram_assisted()

    if tiktok_data or instagram_data:
        save_data(tiktok_data, instagram_data)

        print()
        print("="*70)
        print("‚úÖ Collection Complete!")
        print("="*70)
        print()
        print("Next steps:")
        print("1. Review/edit CSV files if needed (add missing likes/comments)")
        print("2. Run analysis: python src/main.py")
        print()

if __name__ == "__main__":
    main()
